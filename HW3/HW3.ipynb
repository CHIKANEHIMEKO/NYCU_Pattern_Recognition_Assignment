{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, Random Forest, and Adaboost\n",
    "In hw3, you need to implement decision tree, random forest and adaboost by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "feature_names = data['feature_names']\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"x_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "x_test = pd.read_csv(\"x_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.423</td>\n",
       "      <td>27.88</td>\n",
       "      <td>59.26</td>\n",
       "      <td>271.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.04971</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1742</td>\n",
       "      <td>0.06059</td>\n",
       "      <td>...</td>\n",
       "      <td>10.49</td>\n",
       "      <td>34.24</td>\n",
       "      <td>66.50</td>\n",
       "      <td>330.6</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>0.07158</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.06969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.070</td>\n",
       "      <td>13.44</td>\n",
       "      <td>77.83</td>\n",
       "      <td>445.2</td>\n",
       "      <td>0.11000</td>\n",
       "      <td>0.09009</td>\n",
       "      <td>0.03781</td>\n",
       "      <td>0.02798</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>0.06608</td>\n",
       "      <td>...</td>\n",
       "      <td>13.45</td>\n",
       "      <td>15.77</td>\n",
       "      <td>86.92</td>\n",
       "      <td>549.9</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.16320</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.07393</td>\n",
       "      <td>0.2781</td>\n",
       "      <td>0.08052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.160</td>\n",
       "      <td>26.60</td>\n",
       "      <td>126.20</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.10200</td>\n",
       "      <td>0.14530</td>\n",
       "      <td>0.19210</td>\n",
       "      <td>0.09664</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.06220</td>\n",
       "      <td>...</td>\n",
       "      <td>23.72</td>\n",
       "      <td>35.90</td>\n",
       "      <td>159.80</td>\n",
       "      <td>1724.0</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.38410</td>\n",
       "      <td>0.5754</td>\n",
       "      <td>0.18720</td>\n",
       "      <td>0.3258</td>\n",
       "      <td>0.09720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>19.000</td>\n",
       "      <td>18.91</td>\n",
       "      <td>123.40</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>0.08217</td>\n",
       "      <td>0.08028</td>\n",
       "      <td>0.09271</td>\n",
       "      <td>0.05627</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.05044</td>\n",
       "      <td>...</td>\n",
       "      <td>22.32</td>\n",
       "      <td>25.73</td>\n",
       "      <td>148.20</td>\n",
       "      <td>1538.0</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2841</td>\n",
       "      <td>0.06541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17.540</td>\n",
       "      <td>19.32</td>\n",
       "      <td>115.10</td>\n",
       "      <td>951.6</td>\n",
       "      <td>0.08968</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.07488</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.05491</td>\n",
       "      <td>...</td>\n",
       "      <td>20.42</td>\n",
       "      <td>25.84</td>\n",
       "      <td>139.50</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>0.34200</td>\n",
       "      <td>0.3508</td>\n",
       "      <td>0.19390</td>\n",
       "      <td>0.2928</td>\n",
       "      <td>0.07867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        9.423         27.88           59.26      271.3          0.08123   \n",
       "1       12.070         13.44           77.83      445.2          0.11000   \n",
       "2       19.160         26.60          126.20     1138.0          0.10200   \n",
       "3       19.000         18.91          123.40     1138.0          0.08217   \n",
       "4       17.540         19.32          115.10      951.6          0.08968   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.04971         0.00000              0.00000         0.1742   \n",
       "1           0.09009         0.03781              0.02798         0.1657   \n",
       "2           0.14530         0.19210              0.09664         0.1902   \n",
       "3           0.08028         0.09271              0.05627         0.1946   \n",
       "4           0.11980         0.10360              0.07488         0.1506   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.06059  ...         10.49          34.24            66.50   \n",
       "1                 0.06608  ...         13.45          15.77            86.92   \n",
       "2                 0.06220  ...         23.72          35.90           159.80   \n",
       "3                 0.05044  ...         22.32          25.73           148.20   \n",
       "4                 0.05491  ...         20.42          25.84           139.50   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0       330.6            0.1073            0.07158           0.0000   \n",
       "1       549.9            0.1521            0.16320           0.1622   \n",
       "2      1724.0            0.1782            0.38410           0.5754   \n",
       "3      1538.0            0.1021            0.22640           0.3207   \n",
       "4      1239.0            0.1381            0.34200           0.3508   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0               0.00000          0.2475                  0.06969  \n",
       "1               0.07393          0.2781                  0.08052  \n",
       "2               0.18720          0.3258                  0.09720  \n",
       "3               0.12180          0.2841                  0.06541  \n",
       "4               0.19390          0.2928                  0.07867  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from the course sludes on E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini impurity: https://kknews.cc/zh-tw/code/443je5v.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    g = 0\n",
    "    count = np.bincount(sequence)\n",
    "    count_arr = count[np.nonzero(count)]\n",
    "    n = count_arr.sum()\n",
    "    for i in count_arr:\n",
    "        g = g + (i/n)**2\n",
    "    gini = 1.0 - g\n",
    "    return gini\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    e = 0\n",
    "    count = np.bincount(sequence)\n",
    "    count_arr = count[np.nonzero(count)]\n",
    "    n = count_arr.sum()\n",
    "    for i in count_arr:\n",
    "        e = e - (i/n)*math.log2(i/n)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006402\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.5\n",
      "Entropy of data is  1.0\n"
     ]
    }
   ],
   "source": [
    "# Test gini & entrophy function by myself\n",
    "data2 = np.array([1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n",
    "print(\"Gini of data is \", gini(data2))\n",
    "print(\"Entropy of data is \", entropy(data2))\n",
    "\n",
    "# I think the outcome is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.0\n",
      "Entropy of data is  0.0\n"
     ]
    }
   ],
   "source": [
    "# Test gini & entrophy function by myself\n",
    "data2 = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "print(\"Gini of data is \", gini(data2))\n",
    "print(\"Entropy of data is \", entropy(data2))\n",
    "\n",
    "# I think the outcome is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **Criterion**: The function to measure the quality of a split. Your model should support “gini” for the Gini impurity and “entropy” for the information gain. \n",
    "2. **Max_depth**: The maximum depth of the tree. If Max_depth=None, then nodes are expanded until all leaves are pure. Max_depth=1 equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stack to record the depth of tree\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class Stack:\n",
    "    def __init__(self, initial_data):\n",
    "        self.stack = []\n",
    "        self.initial_data = initial_data\n",
    "\n",
    "        # Is initial_data iterable ?\n",
    "        if isinstance(initial_data, Iterable):\n",
    "            self.stack = list(initial_data)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Initial＿data was not iterable data')\n",
    "    # python Interpreter\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Stack(initial_data={!r})\".format(self.initial_data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"stack({})\".format(self.stack)\n",
    "\n",
    "    # return: int\n",
    "    def __len__(self):\n",
    "        return len(self.stack)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.stack[i]\n",
    "\n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return len(self.stack) == 0\n",
    "\n",
    "    def push(self, data):\n",
    "        self.stack.append(data)\n",
    "\n",
    "    # return: data that pop out\n",
    "    def pop(self):\n",
    "        if not self.is_empty:\n",
    "            return self.stack.pop()\n",
    "\n",
    "    # return: top element in stack\n",
    "    def peek(self):\n",
    "        return self.stack[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.target_class = np.argmax(np.bincount(np.array(label.iloc[:, 0])))\n",
    "\n",
    "\n",
    "class Decision_Node:\n",
    "    def __init__(self,\n",
    "                 question,\n",
    "                 true_branch,\n",
    "                 false_branch,\n",
    "                 threshold):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "        self.threshold = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.depth_record = []\n",
    "        self.stack = Stack(self.depth_record)\n",
    "        self.stack.push(1)\n",
    "        self.n = 0\n",
    "        self.total = 0\n",
    "        return None\n",
    "\n",
    "    def questions(self, data, label):\n",
    "        best_gain = 0  # keep track of the best information gain\n",
    "        best_question = None  # keep train of the feature / value that produced it\n",
    "        #current_uncertainty = gini(rows)\n",
    "        features_num = data.shape[1]\n",
    "\n",
    "        for i in range(features_num):\n",
    "            col = pd.DataFrame(data.iloc[:, i])\n",
    "            left, right, left_label, right_label, threshold = self.partition(\n",
    "                data, list(col.columns), label)\n",
    "            # Skip this split if it doesn't divide the dataset.\n",
    "            if left.shape[0] == 0 or right.shape[0] == 0:\n",
    "                continue\n",
    "            # Calculate the information gain from this split\n",
    "            p = float(left.shape[0]) / (left.shape[0] + right.shape[0])\n",
    "            left_arr = np.array(left_label.iloc[:, 0])\n",
    "            right_arr = np.array(right_label.iloc[:, 0])\n",
    "\n",
    "            if self.criterion == 'gini':\n",
    "                gain = gini(\n",
    "                    np.array(label.iloc[:, 0])) - p * gini(left_arr) - (1 - p) * gini(right_arr)\n",
    "            else:\n",
    "                gain = entropy(np.array(\n",
    "                    label.iloc[:, 0])) - p * entropy(left_arr) - (1 - p) * entropy(right_arr)\n",
    "            if gain >= best_gain:\n",
    "\n",
    "                best_gain, best_question = gain, list(col.columns)\n",
    "\n",
    "        return best_gain, best_question\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        gain, question = self.questions(X, y)\n",
    "        n = self.stack.pop()\n",
    "        if gain == 0 or n == self.max_depth:\n",
    "            return Leaf(X, y)\n",
    "        self.stack.push(n+1)\n",
    "        self.stack.push(n+1)\n",
    "        left, right, label_l, label_r, threshold = self.partition(\n",
    "            X, question, y)\n",
    "        left_branch = self.fit(left, label_l)\n",
    "        right_branch = self.fit(right, label_r)\n",
    "        return Decision_Node(question, left_branch, right_branch, threshold)\n",
    "\n",
    "    def partition(self, data, question, label):\n",
    "        true_rows, false_rows = [], []\n",
    "        col = data[question]\n",
    "        criteria = float(col.mean())\n",
    "        for ind in range(len(col)):\n",
    "            if float(col.iloc[ind]) >= criteria:\n",
    "                true_rows.append(ind)\n",
    "            else:\n",
    "                false_rows.append(ind)\n",
    "        left = data.iloc[true_rows, :]\n",
    "        label_l = label.iloc[true_rows, :]\n",
    "        right = data.iloc[false_rows, :]\n",
    "        label_r = label.iloc[false_rows, :]\n",
    "        return left, right, label_l, label_r, criteria\n",
    "\n",
    "    def partition_test(self, data, label, question, criteria):\n",
    "        true_rows, false_rows = [], []\n",
    "        col = data[question]\n",
    "        for ind in range(len(col)):\n",
    "            if float(col.iloc[ind]) >= criteria:\n",
    "                true_rows.append(ind)\n",
    "            else:\n",
    "                false_rows.append(ind)\n",
    "        left = data.iloc[true_rows, :]\n",
    "        label_l = label.iloc[true_rows, :]\n",
    "        right = data.iloc[false_rows, :]\n",
    "        label_r = label.iloc[false_rows, :]\n",
    "        return left, right, label_l, label_r\n",
    "\n",
    "    def prediction(self, tree, test, label):\n",
    "        if isinstance(tree, Leaf):\n",
    "            tree.data = test\n",
    "            tree.label = label\n",
    "            return\n",
    "        # partition the test data by question and criteria\n",
    "        left, right, label_l, label_r = self.partition_test(\n",
    "            test, label, tree.question, tree.threshold)\n",
    "        if left.empty or right.empty:\n",
    "            return Leaf(test, label)\n",
    "        self.prediction(tree.true_branch, left, label_l)\n",
    "        self.prediction(tree.false_branch, right, label_r)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb?fbclid=IwAR3niNPEZ5a36SUjaNXn2BbbDwlVnRkfGER5lPPDo_arrQWXEDFfC7wmu1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "total = 0\n",
    "arr = 0.0\n",
    "\n",
    "\n",
    "def accuracy_count(node):\n",
    "    # Base case: we've reached a leaf\n",
    "    global n, total, arr\n",
    "    if isinstance(node, Leaf):\n",
    "        total = total + len(np.array(node.label.iloc[:, 0]))\n",
    "        c = np.array(node.label.iloc[:, 0])\n",
    "        n = n + np.max(np.bincount(c))\n",
    "        arr = float(n/total)\n",
    "        return\n",
    "    accuracy_count(node.true_branch)\n",
    "    accuracy_count(node.false_branch)\n",
    "\n",
    "\n",
    "def accuracy(node):\n",
    "    global n, total, arr\n",
    "    n = 0\n",
    "    total = 0\n",
    "    arr = 0\n",
    "    accuracy_count(node)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, spacing=\"\"):\n",
    "\n",
    "   # 要改為印出 label 數量\n",
    "\n",
    "    # Base case: we've reached a leaf\n",
    "    if isinstance(node, Leaf):\n",
    "        print(spacing + \"Leaf\", collections.Counter(\n",
    "            np.array(node.label.iloc[:, 0])), \" ,class:\", node.target_class)\n",
    "        return\n",
    "\n",
    "    # Print the question at this node\n",
    "    print(spacing + str(node.question), node.threshold)\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print(spacing + '--> left')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print(spacing + '--> right:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using Criterion=‘gini’, showing the accuracy score of test data by Max_depth=3 and Max_depth=10, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916083916083916"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "tree_3 = clf_depth3.fit(x_train, y_train)\n",
    "clf_depth3.prediction(tree_3, x_test, y_test)\n",
    "accuracy(tree_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worst area'] 889.1626760563385\n",
      "--> left\n",
      "  ['worst concave points'] 0.18283400000000008\n",
      "  --> left\n",
      "    Leaf Counter({0: 20})  ,class: 0\n",
      "  --> right:\n",
      "    Leaf Counter({0: 25, 1: 1})  ,class: 0\n",
      "--> right:\n",
      "  ['worst concavity'] 0.1884026941580757\n",
      "  --> left\n",
      "    Leaf Counter({1: 36, 0: 10})  ,class: 1\n",
      "  --> right:\n",
      "    Leaf Counter({1: 50, 0: 1})  ,class: 1\n"
     ]
    }
   ],
   "source": [
    "print_tree(tree_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96875"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "tree_10 = clf_depth10.fit(x_train, y_train)\n",
    "clf_depth10.prediction(tree_10, x_test, y_test)\n",
    "accuracy(tree_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list.append(tree_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list.append(tree_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using Max_depth=3, showing the accuracy score of test data by Criterion=‘gini’ and Criterion=’entropy’, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916083916083916"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "tree_g = clf_gini.fit(x_train, y_train)\n",
    "clf_gini.prediction(tree_g, x_test, y_test)\n",
    "accuracy(tree_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916083916083916"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "tree_e = clf_entropy.fit(x_train, y_train)\n",
    "clf_entropy.prediction(tree_e, x_test, y_test)\n",
    "accuracy(tree_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: All of your accuracy scores should over 0.9\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the feature counts for building tree without normalize the importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = []\n",
    "\n",
    "\n",
    "def print_tree_feature(node, spacing=\"\"):\n",
    "    global question_list\n",
    "    if isinstance(node, Leaf):\n",
    "        return\n",
    "\n",
    "    question_list.append(str(node.question))\n",
    "\n",
    "    # Call this function recursively on the true branch\n",
    "    print_tree_feature(node.true_branch, spacing + \"  \")\n",
    "\n",
    "    # Call this function recursively on the false branch\n",
    "    print_tree_feature(node.false_branch, spacing + \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['worst area']\",\n",
       " \"['worst concave points']\",\n",
       " \"['worst concave points']\",\n",
       " \"['mean texture']\",\n",
       " \"['worst symmetry']\",\n",
       " \"['worst concavity']\",\n",
       " \"['worst fractal dimension']\",\n",
       " \"['worst concavity']\",\n",
       " \"['worst concave points']\",\n",
       " \"['worst texture']\",\n",
       " \"['worst symmetry']\",\n",
       " \"['smoothness error']\",\n",
       " \"['worst concavity']\",\n",
       " \"['worst concave points']\",\n",
       " \"['worst texture']\",\n",
       " \"['mean smoothness']\",\n",
       " \"['worst smoothness']\",\n",
       " \"['worst compactness']\",\n",
       " \"['worst concave points']\",\n",
       " \"['mean texture']\",\n",
       " \"['worst area']\",\n",
       " \"['worst concavity']\",\n",
       " \"['worst symmetry']\",\n",
       " \"['area error']\",\n",
       " \"['mean fractal dimension']\",\n",
       " \"['concave points error']\",\n",
       " \"['area error']\",\n",
       " \"['worst concave points']\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_tree_feature(tree_10)\n",
    "question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"['worst area']\": 2,\n",
       "         \"['worst concave points']\": 6,\n",
       "         \"['mean texture']\": 2,\n",
       "         \"['worst symmetry']\": 3,\n",
       "         \"['worst concavity']\": 4,\n",
       "         \"['worst fractal dimension']\": 1,\n",
       "         \"['worst texture']\": 2,\n",
       "         \"['smoothness error']\": 1,\n",
       "         \"['mean smoothness']\": 1,\n",
       "         \"['worst smoothness']\": 1,\n",
       "         \"['worst compactness']\": 1,\n",
       "         \"['area error']\": 2,\n",
       "         \"['mean fractal dimension']\": 1,\n",
       "         \"['concave points error']\": 1})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "feature_dic = collections.Counter(question_list)\n",
    "feature_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = sorted(feature_dic.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "x, y = zip(*dic)\n",
    "plt.barh(x, y, align='center', alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **N_estimators**: The number of trees in the forest. \n",
    "2. **Max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **Bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=J4Wdy0Wc_xQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rstudio-pubs-static.s3.amazonaws.com/378052_30d987a09ea54b6db5aa1e82f5dce6bf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.depth_record = []\n",
    "        self.stack = Stack(self.depth_record)\n",
    "        self.stack.push(1)\n",
    "        self.forest = []\n",
    "        self.n = 0\n",
    "        self.total = 0\n",
    "        return None\n",
    "\n",
    "    # If bootstrap=False, then each tree is built on all training samples.\n",
    "    # I set 35% data randomly choosed when bootstrap == True\n",
    "    def data_bootstrap(self, data):\n",
    "        return data.sample(frac=1.0, replace=True)\n",
    "\n",
    "    # randomly select n features and choose the suitable feature from these n data\n",
    "    def choose_feature(self,):\n",
    "        feature_list = np.random.choice(feature_names, size=int(\n",
    "            self.max_features), replace=False, p=None)\n",
    "        return feature_list\n",
    "\n",
    "    def questions(self, data, label):\n",
    "        best_gain = 0  # keep track of the best information gain\n",
    "        best_question = None  # keep train of the feature / value that produced it\n",
    "        #current_uncertainty = gini(rows)\n",
    "        features = self.choose_feature()\n",
    "\n",
    "        for i in range(len(features)):\n",
    "            col = data[[features[i]]]\n",
    "            left, right, left_label, right_label, threshold = self.partition(\n",
    "                data, list(col.columns), label)\n",
    "            # Skip this split if it doesn't divide the dataset.\n",
    "            if left.shape[0] == 0 or right.shape[0] == 0:\n",
    "                continue\n",
    "            # Calculate the information gain from this split\n",
    "            p = float(left.shape[0]) / (left.shape[0] + right.shape[0])\n",
    "            left_arr = np.array(left_label.iloc[:, 0])\n",
    "            right_arr = np.array(right_label.iloc[:, 0])\n",
    "\n",
    "            if self.criterion == 'gini':\n",
    "                gain = gini(\n",
    "                    np.array(label.iloc[:, 0])) - p * gini(left_arr) - (1 - p) * gini(right_arr)\n",
    "            else:\n",
    "                gain = entropy(np.array(\n",
    "                    label.iloc[:, 0])) - p * entropy(left_arr) - (1 - p) * entropy(right_arr)\n",
    "\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, list(col.columns)\n",
    "        return best_gain, best_question\n",
    "\n",
    "    def partition(self, data, question, label):\n",
    "        true_rows, false_rows = [], []\n",
    "        col = data[question]\n",
    "        criteria = float(col.mean())\n",
    "        for ind in range(len(col)):\n",
    "            if float(col.iloc[ind]) >= criteria:\n",
    "                true_rows.append(ind)\n",
    "            else:\n",
    "                false_rows.append(ind)\n",
    "        left = data.iloc[true_rows, :]\n",
    "        label_l = label.iloc[true_rows, :]\n",
    "        right = data.iloc[false_rows, :]\n",
    "        label_r = label.iloc[false_rows, :]\n",
    "        return left, right, label_l, label_r, criteria\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        gain, question = self.questions(X, y)\n",
    "        n = self.stack.pop()\n",
    "        if gain == 0 or n == self.max_depth:\n",
    "            return Leaf(X, y)\n",
    "        self.stack.push(n+1)\n",
    "        self.stack.push(n+1)\n",
    "        left, right, label_l, label_r, threshold = self.partition(\n",
    "            X, question, y)\n",
    "        left_branch = self.fit(left, label_l)\n",
    "        right_branch = self.fit(right, label_r)\n",
    "        return Decision_Node(question, left_branch, right_branch, threshold)\n",
    "\n",
    "    def build_forest(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            self.depth_record = []\n",
    "            self.stack = Stack(self.depth_record)\n",
    "            self.stack.push(1)\n",
    "            if self.boostrap == True:\n",
    "                data = self.data_bootstrap(X)\n",
    "                index = np.array(data.index.tolist())\n",
    "                label = y.iloc[index, :]\n",
    "                self.forest.append(self.fit(data, label))\n",
    "            else:\n",
    "                self.forest.append(self.fit(X, y))\n",
    "        return self.forest\n",
    "\n",
    "    # input data row by row\n",
    "    def prediction(self, tree, data, sequence):\n",
    "        if isinstance(tree, Leaf):\n",
    "            sequence.append(tree.target_class)\n",
    "            return\n",
    "        # partition the test data by question and criteria\n",
    "        ans = self.partition_test(data, tree.question, tree.threshold)\n",
    "        if ans == 'left':\n",
    "            self.prediction(tree.true_branch, data, sequence)\n",
    "        else:\n",
    "            self.prediction(tree.false_branch, data, sequence)\n",
    "        return\n",
    "\n",
    "    def vote(self, forest, test):\n",
    "        ans_list = []\n",
    "        n = None\n",
    "        for k in range(test.shape[0]):\n",
    "            vote_list = []\n",
    "            for i in range(len(forest)):\n",
    "                self.prediction(forest[i], test.iloc[k:k+1, :], vote_list)\n",
    "            n = np.argmax(np.bincount(vote_list))\n",
    "            ans_list.append(int(n))\n",
    "        return ans_list\n",
    "\n",
    "    def partition_test(self, data, question, criteria):\n",
    "        col = data[question]\n",
    "        if float(np.array(col)) >= float(criteria):\n",
    "            return 'left'\n",
    "        else:\n",
    "            return 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_accuracy(pre, test):\n",
    "    test = np.array(test).flatten()\n",
    "    pre = np.array(pre)\n",
    "    count = 0\n",
    "    n = len(pre)\n",
    "    for i in range(n):\n",
    "        if pre[i] == test[i]:\n",
    "            count = count+1\n",
    "    return float(count/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8741258741258742"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing\n",
    "try1 = RandomForest(n_estimators=3, max_features=5,\n",
    "                    boostrap=True, criterion='gini', max_depth=3)\n",
    "tree1 = try1.build_forest(x_train, y_train)\n",
    "final = try1.vote(tree1, x_test)\n",
    "forest_accuracy(final, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Using Criterion=‘gini’, Max_depth=None, Max_features=sqrt(n_features), showing the accuracy score of test data by n_estimators=10 and n_estimators=100, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_10tree = RandomForest(\n",
    "    n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
    "tree10 = clf_10tree.build_forest(x_train, y_train)\n",
    "final10 = clf_10tree.vote(tree10, x_test)\n",
    "forest_accuracy(final10, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_100tree = RandomForest(\n",
    "    n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
    "tree100 = clf_100tree.build_forest(x_train, y_train)\n",
    "final100 = clf_100tree.vote(tree100, x_test)\n",
    "forest_accuracy(final100, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "Using Criterion=‘gini’, Max_depth=None, N_estimators=10, showing the accuracy score of test data by Max_features=sqrt(n_features) and Max_features=n_features, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(\n",
    "    x_train.shape[1]))\n",
    "random_tree = clf_random_features.build_forest(x_train, y_train)\n",
    "final_random = clf_random_features.vote(random_tree, x_test)\n",
    "forest_accuracy(final_random, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_all_features = RandomForest(\n",
    "    n_estimators=10, max_features=x_train.shape[1])\n",
    "random_all = clf_all_features.build_forest(x_train, y_train)\n",
    "final_all = clf_all_features.vote(random_all, x_test)\n",
    "forest_accuracy(final_all, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
